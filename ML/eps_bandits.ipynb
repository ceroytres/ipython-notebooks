{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are at favorite casino a are $k$-different slot machines. Each machine has ouput a reward according to an unknown distribution. In multi-armed bandit problem, an agent (or a player) has $k$ different actions with an associated expected reward value. Suppose $A_t=a$ denotes the action taken time $t$ and a associated reward $R_t$. The value of action $a$ is denoted by:\n",
    "$$q(a) = \\mathbb{E}[R_t|A_t=a] $$\n",
    "Since the distribution is unknown to the agent, the estimate of $q(a)$ required which will be denoted by $Q(a)$.\n",
    "\n",
    "### Goal\n",
    "The goal of any gambler is to minimize regret, and similarly an agent faced with a multi-armed bandit problem wants to minimize regret. Regret $\\rho$ after $T$ rounds is defined as follows:\n",
    "\n",
    "$$\\rho=Tq^{\\star}-\\sum_{t=1}^T R_t $$\n",
    "\n",
    "where $q^{\\star} = \\max_a q(a)$. The regret metric accounts for the disparity between the maximal expected reward overall $q(a)$ and the accumulated rewards.\n",
    "\n",
    "### Approach \n",
    "\n",
    "A plausible approach could be a greed approach, in which given the current estimates of $Q(a)$, the action is decided via the following policy:\n",
    "\n",
    "$$A_t=\\text{argmax}_aQ(a)$$\n",
    "\n",
    "Such a strategy does not allow for exploration of different actions and also, the agent becomes dependent on initial conditions. Instead we allow the agent to make a random selection among the $k$ different actions with a probability $\\varepsilon$. Such algorithms are dubbed $\\varepsilon$-greedy algorithms\n",
    "\n",
    "### Estimating $Q(a)$\n",
    "\n",
    "The initial estimate of $Q(a)$ can be initialized to encode prior beliefs. Since $Q(a)$ is an estimate of $q(a)$, it makes sense that the sample mean of the rewards be a sufficient estimator of $q(a)$. An efficient streaming algorithm for estimating the sample mean is given by:\n",
    "\n",
    "$$N(A) \\gets N(a)+1 $$\n",
    "\n",
    "$$Q(A) \\gets Q(a) + \\frac{1}{N(a)} (R-Q(a)) $$\n",
    "\n",
    "where $N(A)$ is number of occurrences of action A.\n",
    "\n",
    "### $\\varepsilon$-Greedy Algorithm\n",
    "\n",
    "1. Initialize for $k=1$ to $a$:\n",
    "    2.         $Q(A)\\gets 0$\n",
    "    3.         $N(A)\\gets 0$\n",
    "2. for $t=1$ to $T$ rounds:\n",
    "\n",
    "    A.$A_t\\gets \\begin{cases} \\text{argmax}_a Q(a) \\quad \\text{with probability } 1-\\varepsilon \\\\\n",
    "        \\text{a random action} \\quad \\text{with probability } \\varepsilon \\end{cases}$\n",
    "        \n",
    "    B.$R_t \\gets \\text{bandit}(A)$\n",
    "    \n",
    "    C.$N(A) \\gets N(a)+1 $\n",
    "    \n",
    "    D.$Q(A) \\gets Q(a) + \\frac{1}{N(a)} (R_t-Q(a)) $\n",
    "    \n",
    "    \n",
    "### Applications\n",
    "\n",
    "Multi-Armed bandits appear in the applications such as A/B testing, portfolio design, and etc. For more information, see references [2] and [3].\n",
    "\n",
    "### Experiments\n",
    "\n",
    "For the experiments below, the bandit will have $k=10$ actions. The bandit will output a reward with the following distribution $\\mathcal{N}(\\frac{a}{10},1)$ for the $a^{\\text{th}}$ action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "class EpsilonGreedy_Strategy(object):\n",
    "    def __init__(self,k,eps):\n",
    "        self.k=k\n",
    "        self.Q=np.zeros(k)\n",
    "        self.N=np.zeros(k)\n",
    "        self.eps=eps\n",
    "        \n",
    "    def update(self,R,a):\n",
    "        self.N[a]+=1\n",
    "        self.Q[a]=self.Q[a]+(1./self.N[a])*(R-self.Q[a])\n",
    "    \n",
    "    def choose_action(self):\n",
    "        if np.random.rand()<1-self.eps:\n",
    "            return self.Q.argmax()\n",
    "        else:\n",
    "            return np.random.randint(0,k)\n",
    "\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    k=10\n",
    "    R=(np.array(range(k)))/float(k)\n",
    "    \n",
    "    def bandit_k(a,R):\n",
    "        return np.random.randn()+R[a]\n",
    "    \n",
    "    \n",
    "    \n",
    "    base_eps=float(1)\n",
    "    T=int(1e6)\n",
    "    runs=7\n",
    "    A_player=[]\n",
    "    R_player=[]\n",
    "    \n",
    "    for r in xrange(0,runs):\n",
    "        A_player.append(np.zeros(T))\n",
    "        R_player.append(np.zeros(T))\n",
    "        eps=base_eps/np.power(10,r)\n",
    "        player=EpsilonGreedy_Strategy(k,eps)\n",
    "        for i in xrange(0,T):\n",
    "            A=player.choose_action()\n",
    "            A_player[r][i]=A\n",
    "            R_out=bandit_k(A,R)\n",
    "            R_player[r][i]=R_out\n",
    "            player.update(R_out,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(15,10))\n",
    "    for r in xrange(runs):\n",
    "        eps=base_eps/np.power(10,r)\n",
    "        plt.hist(A_player[r]+1,bins=10,label=r'$\\varepsilon='+str(eps)+'$',alpha=1./(r+1))\n",
    "    plt.xlabel(r'Iterations $t$')\n",
    "    plt.ylabel(r'Action $a_t$')\n",
    "    plt.title('Histogram of Actions Taken by Agent')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    for r in xrange(runs):\n",
    "        eps=base_eps/np.power(10,r)\n",
    "        tot_R=R_player[r].cumsum()\n",
    "        plt.semilogx(tot_R,label=r'$\\varepsilon='+str(eps)+'$')\n",
    "    plt.xlabel(r'Iterations $t$')\n",
    "    plt.ylabel(r'Reward $R_t$')\n",
    "    plt.title('Total Rewards Collected by Agent')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    for r in xrange(runs):\n",
    "        eps=base_eps/np.power(10,r)\n",
    "        tot_R=R_player[r].cumsum()\n",
    "        avg_R=tot_R/(np.arange(1,T+1)*1.)\n",
    "        plt.semilogx(avg_R,label=r'$\\varepsilon='+str(eps)+'$')\n",
    "    plt.xlabel(r'Iterations $t$')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.ylabel(r'Average Reward $\\bar{R}_t$')\n",
    "    plt.title('Average Rewards Collected by Agent')\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### References:\n",
    "\n",
    "1. Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction Second Edition\n",
    "2. https://support.google.com/analytics/answer/2844870?hl=en\n",
    "3. https://en.wikipedia.org/wiki/Multi-armed_bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
